{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db6282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf30fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bertopic import BERTopic \n",
    "from sentence_transformers import SentenceTransformer, util \n",
    "from umap import UMAP \n",
    "from hdbscan import HDBSCAN \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from bertopic.vectorizers import ClassTfidfTransformer \n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')  # This is the Punkt sentence tokenizer.\n",
    "\n",
    "# This will contain each sentence as a separate document.\n",
    "sentences_as_documents_essay = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0346b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33354b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(file):\n",
    "    # Load text\n",
    "    with open(file, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Apply regex preprocessing (if necessary)\n",
    "    # regex how to custom it\n",
    "    regex = r\"\\n[A-Z]+[\\s&]*[A-Z\\s]+\\s*[0-9]\\s*\\n\\n(.+?)\\n\\n\\n\"\n",
    "    content_each = re.findall(regex, text)\n",
    "    sentences_as_documents_essay = []\n",
    "    \n",
    "    for document in content_each:\n",
    "    # Tokenize the current document into sentences.\n",
    "        sentences = nltk.sent_tokenize(document)\n",
    "        sentences_as_documents_essay.extend(sentences)\n",
    "    \n",
    "    return sentences_as_documents_essay  # Or whatever is the processed format\n",
    "\n",
    "def extract_topics(processed_text):\n",
    "    #how to find the approrate stopwords\n",
    "    custom_stopwords=['berenices','lares','penates','inhabitants','city','cities','diomira', 'isidora', 'dorothea', 'zaira', 'anastasia', 'tamara', 'zora', 'despina', 'zirma', 'isaura', 'maurilia', 'fedora', 'zoe', 'zenobia', 'euphemia', 'zobeide', 'hypatia', 'armilla', 'chloe', 'valdrada', 'olivia', 'sophronia', 'eutropia', 'zemrude', 'aglaura', 'octavia', 'ersilia', 'baucis', 'leandra', 'melania', 'esmeralda', 'phyllis', 'pyrrha', 'adelma', 'eudoxia', 'moriana', 'clarice', 'eusapia', 'beersheba', 'leonia', 'irene', 'argia', 'thekla', 'trude', 'olinda', 'laudomia', 'perinthia', 'procopia', 'raissa', 'andria', 'cecilia', 'marozia', 'penthesilea', 'theodora', 'berenice']\n",
    "    all_stopwords=list(ENGLISH_STOP_WORDS)+ custom_stopwords\n",
    "    vectorizer_model = CountVectorizer(stop_words= all_stopwords ) \n",
    "\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    representation_model = KeyBERTInspired()\n",
    "    sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = sentence_model.encode(processed_text, show_progress_bar=True)\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, \n",
    "                      min_dist=0.0, metric='cosine', random_state=41)\n",
    "\n",
    "    topic_model = BERTopic(vectorizer_model=vectorizer_model,\n",
    "                          ctfidf_model=ctfidf_model,\n",
    "                          representation_model=representation_model,\n",
    "                          embedding_model=sentence_model,\n",
    "                          umap_model=umap_model)\n",
    "    topics, probs = topic_model.fit_transform(processed_text)\n",
    "    \n",
    "    df = pd.DataFrame({\"Document\": processed_text, \"Topic\": topics})\n",
    "    \n",
    "    document_topic_pairs = [(doc, topic) for doc, topic in zip(df['Document'], df['Topic'])]\n",
    "    \n",
    "    return document_topic_pairs\n",
    "\n",
    "def BERTopic_result(file):\n",
    "    load_and_preprocess(file)\n",
    "    return extract_topics(load_and_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301aa04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
